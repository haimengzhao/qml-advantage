{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import keras\n",
    "from quantum_model import Quantum_Strategy\n",
    "from tqdm import tqdm\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n):\n",
    "    data = np.load(f\"data_{n}.npz\")\n",
    "    data_size = len(data['X'])\n",
    "    x_train = data['X']; y_train = data['Y']\n",
    "    x_test = np.random.randint(0, 2, (1000, x_train.shape[1]))\n",
    "    # one-hot\n",
    "    x_train = keras.utils.to_categorical(x_train, 2)\n",
    "    x_test = keras.utils.to_categorical(x_test, 2)\n",
    "    # 0, 1, start=2, end=3\n",
    "    y_train = np.hstack([np.ones((data_size, 1)) * 2, y_train, np.ones((data_size, 1)) * 3])\n",
    "    y_train = keras.utils.to_categorical(y_train, 4)\n",
    "    decoder_input_data = y_train[:, :-1]\n",
    "    decoder_target_data = y_train[:, 1:]\n",
    "    print(\"n=\", n)\n",
    "    print(\"n_qubits=\", 4 * n)\n",
    "    print(\"x_train shape:\", x_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(x_train.shape[0], \"train samples\")\n",
    "    print(x_test.shape[0], \"test samples\")\n",
    "    max_decoder_seq_length = decoder_input_data.shape[1]\n",
    "    return x_train, x_test, decoder_input_data, decoder_target_data, max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_and_train(n, latent_dim, x_train, decoder_input_data, decoder_target_data, verbose=0):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = keras.Input(shape=(None, 2))\n",
    "    encoder = keras.layers.GRU(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = state_h\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = keras.Input(shape=(None, 4))\n",
    "\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = keras.layers.Dense(4, activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    )\n",
    "    batch_size = 1000\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(filepath=f\"model_n{n}_l{latent_dim}.keras\", save_best_only=True),\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=500),\n",
    "        TqdmCallback(verbose=0)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        [x_train, decoder_input_data],\n",
    "        decoder_target_data,\n",
    "        batch_size=batch_size,\n",
    "        epochs=10000,\n",
    "        validation_split=0.1,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_predict(n, latent_dim, test_size, max_decoder_seq_length, x_train, x_test):\n",
    "    # Define sampling models\n",
    "    # Restore the model and construct the encoder and decoder.\n",
    "    model = keras.models.load_model(f\"model_n{n}_l{latent_dim}.keras\")\n",
    "\n",
    "    encoder_inputs = model.input[0]  # input_1\n",
    "    encoder_outputs, state_h_enc = model.layers[2].output  # lstm_1\n",
    "    encoder_states = state_h_enc\n",
    "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_inputs = model.input[1]  # input_2\n",
    "    decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = decoder_state_input_h\n",
    "    decoder_lstm = model.layers[3]\n",
    "    decoder_outputs, state_h_dec = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_states = state_h_dec\n",
    "    decoder_dense = model.layers[4]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = keras.Model(\n",
    "        [decoder_inputs] + [decoder_states_inputs], [decoder_outputs] + [decoder_states]\n",
    "    )\n",
    "\n",
    "    def decode_sequence(input_seq):\n",
    "        # Encode the input as state vectors.\n",
    "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "        # Generate empty target sequence of length 1.\n",
    "        target_seq = np.zeros((1, 1, 4))\n",
    "        # Populate the first character of target sequence with the start character 2.\n",
    "        target_seq[0, 0, 2] = 1.0\n",
    "\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "        while not stop_condition:\n",
    "            output_tokens, h = decoder_model.predict(\n",
    "                [target_seq] + [states_value], verbose=0\n",
    "            )\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_char = np.argmax(output_tokens[0, -1, :])\n",
    "            decoded_sentence += [sampled_char]\n",
    "\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if sampled_char == 3 or len(decoded_sentence) > max_decoder_seq_length:\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq = np.zeros((1, 1, 4))\n",
    "            target_seq[0, 0, sampled_char] = 1.0\n",
    "\n",
    "            # Update states\n",
    "            states_value = h\n",
    "        return decoded_sentence\n",
    "    \n",
    "    pred = np.zeros((test_size, x_train.shape[1]))\n",
    "    for seq_index in tqdm(range(test_size)):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = x_test[seq_index : seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        pred[seq_index] = decoded_sentence[:-1]\n",
    "        \n",
    "    qs = Quantum_Strategy(n)\n",
    "    results = qs.check_input_output(np.argmax(x_test[:test_size], axis=-1), pred)\n",
    "    return np.mean(results), np.std(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(n, latent_dim, result):\n",
    "    # add result to a csv file\n",
    "    # file structure: n, latent_dim, result\n",
    "    # where result is a list of past results\n",
    "    \n",
    "    if os.path.exists(\"results.csv\"):\n",
    "        with open(\"results.csv\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) == 0:\n",
    "                lines = []\n",
    "            else:\n",
    "                lines = lines[0].strip().split(\"\\n\")\n",
    "            if len(lines) == 0 or len(lines[-1].split(\",\")) == 3:\n",
    "                lines.append(f\"{n},{latent_dim},{result}\")\n",
    "            else:\n",
    "                lines[-1] += f\",{result}\"\n",
    "    else:\n",
    "        # write header if file does not exist\n",
    "        lines = [\"n,latent_dim,result\"]\n",
    "        lines += [f\"{n},{latent_dim},{result}\"]\n",
    "    with open(\"results.csv\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (10000, 8, 2)\n",
      "y_train shape: (10000, 10, 4)\n",
      "10000 train samples\n",
      "1000 test samples\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0026738643646240234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "epoch",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f235419455064c49b2ea56047c52f9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 242.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "latent_dim = 64\n",
    "test_size = 5000\n",
    "x_train, x_test, decoder_input_data, decoder_target_data, max_decoder_seq_length = load_data(n)\n",
    "model = build_model_and_train(n, latent_dim, x_train, decoder_input_data, decoder_target_data)\n",
    "result = sample_and_predict(n, latent_dim, test_size, max_decoder_seq_length, x_train, x_test)\n",
    "print(result)\n",
    "write_result(n, latent_dim, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_result(n, latent_dim, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
