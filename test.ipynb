{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "import keras\n",
    "from quantum_model import Quantum_Strategy\n",
    "from tqdm import tqdm\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 3\n",
      "n_qubits= 12\n",
      "x_train shape: (10000, 12, 2)\n",
      "y_train shape: (10000, 13, 3)\n",
      "10000 train samples\n",
      "1000 test samples\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "data = np.load(f\"./data/data_{n}.npz\")\n",
    "data_size = len(data['X'])\n",
    "x_train = data['X']; y_train = data['Y']\n",
    "x_test = np.random.randint(0, 2, (1000, x_train.shape[1]))\n",
    "# one-hot\n",
    "x_train = keras.utils.to_categorical(x_train, 2)\n",
    "x_test = keras.utils.to_categorical(x_test, 2)\n",
    "# 0, 1, start=2\n",
    "y_train = np.hstack([np.ones((data_size, 1)) * 2, y_train])\n",
    "y_train = keras.utils.to_categorical(y_train, 3)\n",
    "decoder_input_data = y_train[:, :-1]\n",
    "decoder_target_data = y_train[:, 1:]\n",
    "print(\"n=\", n)\n",
    "print(\"n_qubits=\", 4 * n)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "max_decoder_seq_length = decoder_input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 21:23:09.065589: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.4.99). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 4\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "x_inputs = keras.Input(shape=(None, 2))\n",
    "y_inputs = keras.Input(shape=(None, 3))\n",
    "# stack x and y\n",
    "whole_inputs = keras.layers.Concatenate(axis=-1)([x_inputs, y_inputs])\n",
    "gru = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "gru_outputs, state = gru(whole_inputs)\n",
    "final_outputs = keras.layers.Dense(3, activation=\"softmax\")(gru_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([x_inputs, y_inputs], final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0062580108642578125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "epoch",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405d7c371ebc45428622282f30702411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 21:23:13.096325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f6eaf33feb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
    ")\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=f\"model.keras\", save_best_only=True, monitor='loss'),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"loss\", patience=100),\n",
    "    TqdmCallback(verbose=0)\n",
    "]\n",
    "model.fit(\n",
    "    [x_train, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=1000,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model_pred = keras.models.load_model(f\"model.keras\")\n",
    "\n",
    "x_inputs = model.input[0]  # input_1\n",
    "y_inputs = model.input[1]  # input_2\n",
    "concat = model.layers[2]  # concatenate_1\n",
    "gru = model.layers[3]  # lstm_1\n",
    "init_state = keras.Input(shape=(latent_dim,))\n",
    "gru_outputs, state = gru(concat([x_inputs, y_inputs]), initial_state=init_state)\n",
    "outputs = model.layers[4](gru_outputs)\n",
    "\n",
    "model_pred = keras.Model(\n",
    "    [x_inputs, y_inputs, init_state], [outputs] + [state]\n",
    ")\n",
    "\n",
    "def decode_sequence(x_input):\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 3))\n",
    "    # Populate the first character of target sequence with the start character 2.\n",
    "    target_seq[0, 0, 2] = 1.0\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    init_state = np.zeros((1, latent_dim))\n",
    "    while not stop_condition:\n",
    "        output_tokens, new_state = model_pred.predict(\n",
    "            [x_input[:, [0]]] + [target_seq] + [init_state], verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_char = np.argmax(output_tokens[0, -1, :])\n",
    "        decoded_sentence += [sampled_char]\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if len(decoded_sentence) == max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 3))\n",
    "        target_seq[0, 0, sampled_char] = 1.0\n",
    "        \n",
    "        init_state = new_state\n",
    "        x_input = x_input[:, 1:]\n",
    "\n",
    "    return decoded_sentence\n",
    "    \n",
    "pred = np.zeros((1000, x_train.shape[1]))\n",
    "for seq_index in tqdm(range(1000)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = x_train[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    pred[seq_index] = decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812 0.531\n"
     ]
    }
   ],
   "source": [
    "qs = Quantum_Strategy(n)\n",
    "results = qs.check_input_output(np.argmax(x_train[:1000], axis=-1), pred, flatten=False)\n",
    "checks = np.sum(results, axis=-1) > 0.95 * n\n",
    "print(np.mean(results), np.mean(checks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738763264"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.904**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n):\n",
    "    data = np.load(f\"data_{n}.npz\")\n",
    "    data_size = len(data['X'])\n",
    "    x_train = data['X']; y_train = data['Y']\n",
    "    x_test = np.random.randint(0, 2, (1000, x_train.shape[1]))\n",
    "    # one-hot\n",
    "    x_train = keras.utils.to_categorical(x_train, 2)\n",
    "    x_test = keras.utils.to_categorical(x_test, 2)\n",
    "    # 0, 1, start=2, end=3\n",
    "    y_train = np.hstack([np.ones((data_size, 1)) * 2, y_train, np.ones((data_size, 1)) * 3])\n",
    "    y_train = keras.utils.to_categorical(y_train, 4)\n",
    "    decoder_input_data = y_train[:, :-1]\n",
    "    decoder_target_data = y_train[:, 1:]\n",
    "    print(\"n=\", n)\n",
    "    print(\"n_qubits=\", 4 * n)\n",
    "    print(\"x_train shape:\", x_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(x_train.shape[0], \"train samples\")\n",
    "    print(x_test.shape[0], \"test samples\")\n",
    "    max_decoder_seq_length = decoder_input_data.shape[1]\n",
    "    return x_train, x_test, decoder_input_data, decoder_target_data, max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_and_train(n, latent_dim, x_train, decoder_input_data, decoder_target_data, verbose=0):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = keras.Input(shape=(None, 2))\n",
    "    encoder = keras.layers.GRU(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = state_h\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = keras.Input(shape=(None, 4))\n",
    "\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = keras.layers.Dense(4, activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    )\n",
    "    batch_size = 1000\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(filepath=f\"model_n{n}_l{latent_dim}.keras\", save_best_only=True),\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=500),\n",
    "        TqdmCallback(verbose=0)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        [x_train, decoder_input_data],\n",
    "        decoder_target_data,\n",
    "        batch_size=batch_size,\n",
    "        epochs=100,\n",
    "        validation_split=0.1,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_predict(n, latent_dim, test_size, max_decoder_seq_length, x_train, x_test):\n",
    "    # Define sampling models\n",
    "    # Restore the model and construct the encoder and decoder.\n",
    "    model = keras.models.load_model(f\"model_n{n}_l{latent_dim}.keras\")\n",
    "\n",
    "    encoder_inputs = model.input[0]  # input_1\n",
    "    encoder_outputs, state_h_enc = model.layers[2].output  # lstm_1\n",
    "    encoder_states = state_h_enc\n",
    "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_inputs = model.input[1]  # input_2\n",
    "    decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = decoder_state_input_h\n",
    "    decoder_lstm = model.layers[3]\n",
    "    decoder_outputs, state_h_dec = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_states = state_h_dec\n",
    "    decoder_dense = model.layers[4]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = keras.Model(\n",
    "        [decoder_inputs] + [decoder_states_inputs], [decoder_outputs] + [decoder_states]\n",
    "    )\n",
    "\n",
    "    def decode_sequence(input_seq):\n",
    "        # Encode the input as state vectors.\n",
    "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "        # Generate empty target sequence of length 1.\n",
    "        target_seq = np.zeros((1, 1, 4))\n",
    "        # Populate the first character of target sequence with the start character 2.\n",
    "        target_seq[0, 0, 2] = 1.0\n",
    "\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "        while not stop_condition:\n",
    "            output_tokens, h = decoder_model.predict(\n",
    "                [target_seq] + [states_value], verbose=0\n",
    "            )\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_char = np.argmax(output_tokens[0, -1, :])\n",
    "            decoded_sentence += [sampled_char]\n",
    "\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if sampled_char == 3 or len(decoded_sentence) > max_decoder_seq_length:\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq = np.zeros((1, 1, 4))\n",
    "            target_seq[0, 0, sampled_char] = 1.0\n",
    "\n",
    "            # Update states\n",
    "            states_value = h\n",
    "        return decoded_sentence\n",
    "    \n",
    "    pred = np.zeros((test_size, x_train.shape[1]))\n",
    "    for seq_index in tqdm(range(test_size)):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = x_test[seq_index : seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        pred[seq_index] = decoded_sentence[:-1]\n",
    "        \n",
    "    qs = Quantum_Strategy(n)\n",
    "    results = qs.check_input_output(np.argmax(x_test[:test_size], axis=-1), pred)\n",
    "    return np.mean(results), np.std(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(n, latent_dim, result):\n",
    "    # add results to csv file\n",
    "    # if n, latent_dim not in csv, add new row\n",
    "    # if n, latent_dim in csv, append mean and std at the end of the row\n",
    "    try:\n",
    "        with open(\"results.csv\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "    except:\n",
    "        lines = [\"n,latent_dim,mean,std\\n\"]\n",
    "    with open(\"results.csv\", \"w\") as f:\n",
    "        for line in lines:\n",
    "            if f\"{n},{latent_dim}\" in line:\n",
    "                f.write(line[:-2] + f\",{result[0]},{result[1]}\\n\")\n",
    "            else:\n",
    "                f.write(line)\n",
    "        if not any([f\"{n},{latent_dim}\" in line for line in lines]):\n",
    "            f.write(f\"{n},{latent_dim},{result[0]},{result[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 2\n",
      "n_qubits= 8\n",
      "x_train shape: (10000, 8, 2)\n",
      "y_train shape: (10000, 10, 4)\n",
      "10000 train samples\n",
      "1000 test samples\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0050160884857177734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "epoch",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b75ba8336b41799ccf7046dd17f00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 251.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8095, 0.39269549271668497)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "latent_dim = 64\n",
    "test_size = 1000\n",
    "x_train, x_test, decoder_input_data, decoder_target_data, max_decoder_seq_length = load_data(n)\n",
    "model = build_model_and_train(n, latent_dim, x_train, decoder_input_data, decoder_target_data)\n",
    "result = sample_and_predict(n, latent_dim, test_size, max_decoder_seq_length, x_train, x_test)\n",
    "print(result)\n",
    "write_result(n, latent_dim, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_result(n, latent_dim, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
